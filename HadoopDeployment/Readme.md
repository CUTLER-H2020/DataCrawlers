# Hadoop deployment

This folder has the necessary files to deploy data crawled into Hadoop ecosystem. Please note, there is an example on how to directly injest data from stream (Twitter) at [Social>TwitterData](https://github.com/CUTLER-H2020/DataCrawlers/tree/master/Social/twitterCrawler#direct-injestion-into-hdfs)

## Folder structure 

* [Flume_Config_Sample](Flume_Config_Sample/): Sample config for Flume to ingest data from spool directory to HDFS
* [HDFSTOElasticSearch](HDFSTOElasticSearch/): (Experimental!) Sample script for porting data from HDFS to Elastic Search
